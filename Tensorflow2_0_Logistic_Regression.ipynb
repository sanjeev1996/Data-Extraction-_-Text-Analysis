{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow2.0-Logistic Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOumFo2+i0PbUvzNu9RIx6f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjeev1996/Data-Extraction-_-Text-Analysis/blob/master/Tensorflow2_0_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c41-jbefWJQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RmtFyB8WRCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "rng = np.random\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RToBBQ19W-m3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 10 # 0 to 9 digits \n",
        "num_features = 784 # 28*28\n",
        "\n",
        "# Training dataset parameters\n",
        "learning_rate = 0.01\n",
        "training_steps = 1000\n",
        "batch_size = 250\n",
        "display_step = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYqnuqeCXiGb",
        "colab_type": "code",
        "outputId": "7a0fc1b7-175c-4e5b-dc39-06d7bfa300ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "# Prepare MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Convert to float32\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_train, np.float32)\n",
        "#print(x_test.shape)\n",
        "\n",
        "# Flatten image to 1-D vector of 784 features\n",
        "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
        "# Normalize image from [0,255] to [0,1]\n",
        "x_train, x_test = x_train/255., x_test/255."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAItoXYKY2hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use tf.data API to shuffle and batch data.\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjbIw_9ebogp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weight of shape (784,10), the 28*28 image features, and total number of classe\n",
        "W = tf.Variable(tf.ones([num_features, num_classes]), name='weight')\n",
        "# Bias of hape[10], the total number of classes\n",
        "b = tf.Variable(tf.zeros([num_classes]), name='bias')\n",
        "\n",
        "# Logistic regression (Wx + b) \n",
        "def logistic_regression(x):\n",
        "  # Apply softmax to normalize the logits to a probability distribution.\n",
        "  return tf.nn.softmax(tf.matmul(x,W)+b)\n",
        "\n",
        "# loss function\n",
        "def cross_entropy(y_pred, y_true):\n",
        "  # Encode label to a one hot vector\n",
        "  y_true = tf.one_hot(y_true, depth=num_classes)\n",
        "  # clip predication values to avoid log(0) error\n",
        "  y_pred =tf.clip_by_value(y_pred, 1e-9,1.)\n",
        "  return tf.reduce_mean(-tf.reduce_sum(y_true*tf.math.log(y_pred)))\n",
        "\n",
        "def accuracy(y_pred, y_true):\n",
        "  # highest score in predicton vector\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "534ZNpxD1PJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_optimization(x, y):\n",
        "  # computation insider a gradientTape for automatic differentiation\n",
        "  with tf.GradientTape() as g:\n",
        "    pred = logistic_regression(x)\n",
        "    loss = cross_entropy(pred, y)\n",
        "  \n",
        "  gradients = g.gradient(loss, [W, b])\n",
        "  optimizer.apply_gradients(zip(gradients, [W, b]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdCCjeuv21or",
        "colab_type": "code",
        "outputId": "7cf9f37d-c0b7-4b77-d209-be1714f5e0f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
        "  run_optimization(batch_x, batch_y)\n",
        "  \n",
        "  if step % display_step == 0:\n",
        "    pred = logistic_regression(batch_x)\n",
        "    loss = cross_entropy(pred, batch_y)\n",
        "    acc = accuracy(pred, batch_y)\n",
        "    print(\"step: %i, loss: %f, accuracy: %f,\" %(step, loss, acc))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step: 50, loss: 441.465332, accuracy: 0.784000,\n",
            "step: 100, loss: 605.754456, accuracy: 0.832000,\n",
            "step: 150, loss: 99.215424, accuracy: 0.880000,\n",
            "step: 200, loss: 83.101501, accuracy: 0.936000,\n",
            "step: 250, loss: 78.341766, accuracy: 0.916000,\n",
            "step: 300, loss: 66.827957, accuracy: 0.916000,\n",
            "step: 350, loss: 231.303009, accuracy: 0.788000,\n",
            "step: 400, loss: 139.892288, accuracy: 0.852000,\n",
            "step: 450, loss: 68.784653, accuracy: 0.940000,\n",
            "step: 500, loss: 69.633644, accuracy: 0.904000,\n",
            "step: 550, loss: 103.152855, accuracy: 0.892000,\n",
            "step: 600, loss: 53.666222, accuracy: 0.944000,\n",
            "step: 650, loss: 74.737694, accuracy: 0.908000,\n",
            "step: 700, loss: 37.563000, accuracy: 0.960000,\n",
            "step: 750, loss: 68.825569, accuracy: 0.924000,\n",
            "step: 800, loss: 69.501236, accuracy: 0.928000,\n",
            "step: 850, loss: 57.164696, accuracy: 0.932000,\n",
            "step: 900, loss: 62.723404, accuracy: 0.916000,\n",
            "step: 950, loss: 49.844833, accuracy: 0.940000,\n",
            "step: 1000, loss: 119.397522, accuracy: 0.844000,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q37H-7D6-z6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict image\n",
        "import matplotlib.pyplot as plt\n",
        "n_images = 1\n",
        "test_images = np.array([x_test[0]])\n",
        "print(test_images)\n",
        "predictions = logistic_regression(test_images)\n",
        "print(predictions.numpy())\n",
        "# Display image and model prediction\n",
        "for i in range(n_images):\n",
        "  plt.imshow(np.reshape(test_images, [28,28]), cmap='gray')\n",
        "  plt.show()\n",
        "  print(\"Model prediction: %i\" % np.argmax(predictions.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FFRCyXRVBPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict image\n",
        "import cv2 \n",
        "import numpy as np \n",
        "image = cv2.imread('7.jpg')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 51, 1)\n",
        "resized = cv2.resize(thresh, (28,28), interpolation = cv2.INTER_AREA)\n",
        "test_images = resized.reshape([-1, 784])\n",
        "test_images = test_images/255.\n",
        "test_images = test_images.astype('float32')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "n_images = 1\n",
        "test_images = np.array(test_images)\n",
        "print(test_images)\n",
        "predictions = logistic_regression(test_images)\n",
        "print(predictions.numpy())\n",
        "# Display image and model prediction\n",
        "for i in range(n_images):\n",
        "  plt.imshow(np.reshape(test_images, [28,28]), cmap='gray')\n",
        "  plt.show()\n",
        "  print(\"Model prediction: %i\" % np.argmax(predictions.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}